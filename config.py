"""Default settings for data processing and analysis. Do not edit this file,
but instead create a new configuration changing only the settings you need to
alter for your specific analysis.
"""
import importlib
import functools
import os
import pdb
import traceback
import sys
import copy
import coloredlogs
import logging
from typing import Optional, Union, Iterable
try:
    from typing import Literal
except ImportError:  # Python <3.8
    from typing_extensions import Literal

import numpy as np
import mne
from mne_bids.path import get_entity_vals


study_name: str = ''
"""
Specify the name of your study. It will be used to populate filenames for
saving the analysis results.

???+ example "Example"
    ```python

    study_name = 'my-study'
    ```
"""

bids_root: Optional[str] = None
"""
Speficy the BIDS root directory. Pass an empty string or ```None`` to use
the value specified in the ``BIDS_ROOT`` environment variable instead.
Raises an exception if the BIDS root has not been specified.

???+ example "Example"
    ``` python
    bids_root = '/path/to/your/bids_root'  # Use this to specify a path here.
    bids_root = None  # Make use of the ``BIDS_ROOT`` environment variable.
    ```
"""

subjects_dir: Optional[str] = None
"""
Path to the directory that contains the MRI data files and their
derivativesfor all subjects. Specifically, the ``subjects_dir`` is the
$SUBJECTS_DIR used by the Freesurfer software.
"""

daysback: Optional[int] = None
"""
warning:
     This parameter will soon be removed!
     Anonymization should be done on the BIDS dataset **before** running
     the Study Template!

If not ``None``, apply a time shift to dates to adjust for limitateions
of FIFF files.
"""

interactive: bool = False
"""
If True, the scripts will provide some interactive elements, such as
figures. If running the scripts from a notebook or Spyder,
run `%matplotlib qt` in the command line to open the figures in a separate
window.
"""

crop: Optional[tuple] = None
"""
If tuple, (tmin, tmax) to crop the raw data
If None (default), do not crop.
"""

sessions: Union[Iterable, Literal['all']] = 'all'
"""
The sessions to process.
"""

task: str = ''
"""
The task to process.
"""

runs: Union[Iterable, Literal['all']] = 'all'
"""
The runs to process.
"""

acq: Optional[str] = None
"""
The BIDS `acquisition` entity.
"""

proc: Optional[str] = None
"""
The BIDS `processing` entity.
"""

rec: Optional[str] = None
"""
The BIDS `recording` entity.
"""

space: Optional[str] = None
"""
The BIDS `space` entity.
"""

subjects: Union[Iterable[str], Literal['all']] = 'all'
"""
Subjects to analyze. If ``'all'``, include all subjects. To only
include a subset of subjects, pass a list of their identifiers. Even
if you plan on analyzing only a single subject, pass their identifier
as a list.

Please note that if you intend to EXCLUDE only a few subjects, you
should consider setting ``subjects = 'all'`` and adding the
identifiers of the excluded subjects to ``exclude_subjects`` (see next
section).

???+ example "Example"
    ```python
    subjects = 'all'  # Include all subjects.
    subjects = ['05']  # Only include subject 05.
    subjects = ['01', '02']  # Only include subjects 01 and 02.
    ```
"""

exclude_subjects: Iterable[str] = []
"""
Specify subjects to exclude from analysis. The MEG empty-room mock-subject
is automatically excluded from regular analysis.

???+ info "Good Practice / Advice"
    Keep track of the criteria leading you to exclude
    a participant (e.g. too many movements, missing blocks, aborted experiment,
    did not understand the instructions, etc, ...)
    The ``emptyroom`` subject will be excluded automatically.
"""

process_er: bool = False
"""
Whether to apply the same pre-processing steps to the empty-room data as
to the experimental data (up until including frequency filtering). This
is required if you wish to use the empty-room recording to estimate noise
covariance (via ``noise_cov='emptyroom'``). The empty-room recording
corresponding to the processed experimental data will be retrieved
automatically.
"""

ch_types: Iterable[str] = []
"""
The channel types to consider.

!!! info
    Currently, MEG and EEG data cannot be processed together.

???+ example "Example"
    ```python
    # Use EEG channels:
    ch_types = ['eeg']

    # Use magnetometer and gradiometer MEG channels:
    ch_types = ['mag', 'grad']

    # Currently does not work and will raise an error message:
    ch_types = ['meg', 'eeg']
    ```
"""

data_type: Optional[str] = None
"""
The BIDS data type.

For MEG recordings, this will usually be 'meg'; and for EEG, 'eeg'.
However, if your dataset contains simultaneous recordings of MEG and EEG,
stored in a single file, you will typically need to set this to 'meg'.
If ``None``, we will assume that the data type matches the channel type.

???+ example "Example"
    The dataset contains simultaneous recordings of MEG and EEG, and we only
    wish to process the EEG data, which is stored inside the MEG files:

    ```python
    ch_types = ['eeg']
    data_type = 'eeg'
    ```

    The dataset contains simultaneous recordings of MEG and EEG, and we only wish
    to process the gradiometer data:

    ```python
    ch_types = ['grad']
    data_type = 'meg'  # or data_type = None
    ```

    The dataset contains only EEG data:

    ```python
    ch_types = ['eeg']
    data_type = 'eeg'  # or data_type = None
    ```
"""

eeg_template_montage: Optional[str] = None
"""
In situations where you wish to process EEG data and no individual
digitization points (measured channel locations) are available, you can apply
a "template" montage. This means we will assume the EEG cap was placed
either according to an international system like 10/20, or as suggested by
the cap manufacturers in their respective manual.

Please be aware that the actual cap placement most likely deviated somewhat
from the template, and, therefore, source reconstruction may be impaired.

If ``None``, do not apply a template montage. If a string, must be the
name of a built-in template montage in MNE-Python.
You can find an overview of supported template montages at
https://mne.tools/stable/generated/mne.channels.make_standard_montage.html

???+ example "Example"
    Do not apply template montage:
    ```python
    eeg_template_montage = None
    ```

    Apply 64-channel Biosemi 10/20 template montage:
    ```python
    eeg_template_montage = 'biosemi64'
    ```
"""

###############################################################################
# MAXWELL FILTER PARAMETERS
# -------------------------
# done in 01-import_and_maxfilter.py

find_flat_channels_meg: bool = False
"""
Auto-detect "flat" channels (i.e. those with unusually low variability) and mark
them as bad.
"""

find_noisy_channels_meg: bool = False
"""
Auto-detect "noisy" channels and mark them as bad.
"""

use_maxwell_filter: bool = False
"""
Whether or not to use Maxwell filtering to preprocess the data.

warning:
    If the data were recorded with internal active compensation (MaxShield),
    they need to be run through Maxwell filter to avoid distortions.
    Bad channels need to be set through BIDS channels.tsv and / or via the
    ``find_flat_channels_meg`` and ``find_noisy_channels_meg`` options above
    before applying Maxwell filter.
"""

mf_st_duration: Optional[float] = None
"""
There are two kinds of maxfiltering: SSS (signal space separation) and tSSS
(temporal signal space separation)
(see [Taulu et al., 2004](http://cds.cern.ch/record/709081/files/0401166.pdf)).

If not None, apply spatiotemporal SSS (tSSS) with specified buffer
duration (in seconds). MaxFilter™'s default is 10.0 seconds in v2.2.
Spatiotemporal SSS acts as implicitly as a high-pass filter where the
cut-off frequency is 1/st_dur Hz. For this (and other) reasons, longer
buffers are generally better as long as your system can handle the
higher memory usage. To ensure that each window is processed
identically, choose a buffer length that divides evenly into your data.
Any data at the trailing edge that doesn't fit evenly into a whole
buffer window will be lumped into the previous buffer.

???+ info "Good Practice / Advice"
    If you are interested in low frequency activity (<0.1Hz), avoid using
    tSSS and set ``mf_st_duration`` to ``None``.

    If you are interested in low frequency above 0.1 Hz, you can use the
    default ``mf_st_duration`` to 10 s, meaning it acts like a 0.1 Hz
    high-pass filter.

???+ example "Example"
    ```python
    mf_st_duration = None
    mf_st_duration = 10.  # to apply tSSS with 0.1Hz highpass filter.
    ```
"""

mf_head_origin = 'auto'
"""
``mf_head_origin`` : array-like, shape (3,) | 'auto'
Origin of internal and external multipolar moment space in meters.
If 'auto', it will be estimated from headshape points.
If automatic fitting fails (e.g., due to having too few digitization
points), consider separately calling the fitting function with different
options or specifying the origin manually.

???+ example "Example"
    ```python
    mf_head_origin = 'auto'
    ```
"""

mf_reference_run: Optional[str] = None
"""
Despite all possible care to avoid movements in the MEG, the participant
will likely slowly drift down from the Dewar or slightly shift the head
around in the course of the recording session. Hence, to take this into
account, we are realigning all data to a single position. For this, you need
to define a reference run (typically the one in the middle of
the recording session).

Which run to take as the reference for adjusting the head position of all
runs. If ``None``, pick the first run.

???+ example "Example"
    ```python
    mf_reference_run = '01'  # Use run "01"
    ```
"""

###############################################################################
# STIMULATION ARTIFACT
# --------------------
# used in 01-import_and_maxfilter.py

fix_stim_artifact: bool = False
"""
Apply interpolation to fix stimulation artifact.

???+ example "Example"
    ```python
    fix_stim_artifact = False
    ```
"""

stim_artifact_tmin: float = 0.
"""
Start time of the interpolation window in seconds.

???+ example "Example"
    ```python
    stim_artifact_tmin = 0.  # on stim onset
    ```
"""

stim_artifact_tmax: float = 0.01
"""
End time of the interpolation window in seconds.

???+ example "Example"
    ```python
    stim_artifact_tmax = 0.01  # up to 10ms post-stimulation
    ```
"""

###############################################################################
# FREQUENCY FILTERING
# -------------------
# done in 02-frequency_filter.py

l_freq: float = 1.
"""
The low-frequency cut-off in the highpass filtering step.
Keep it None if no highpass filtering should be applied.
"""

h_freq: float = 40.
"""
The high-frequency cut-off in the lowpass filtering step.
Keep it None if no lowpass filtering should be applied.
"""

###############################################################################
# RESAMPLING
# ----------

resample_sfreq: Optional[float] = None
"""
Specifies at which sampling frequency the data should be resampled.
If None then no resampling will be done.

???+ example "Example"
    ```python
    resample_sfreq = None  # no resampling
    resample_sfreq = 500  # resample to 500Hz
    ```
"""

decim: int = 1
"""
Says how much to decimate data at the epochs level.
It is typically an alternative to the `resample_sfreq` parameter that
can be used for resampling raw data. ``1`` means no decimation.

???+ info "Good Practice / Advice"
    Decimation requires to lowpass filtered the data to avoid aliasing.
    Note that using decimation is much faster than resampling.

???+ example "Example"
    ```python
    decim = 1  # no decimation
    decim = 4  # decimate by 4 ie devide sampling frequency by 4
    ```
"""

###############################################################################
# AUTOMATIC REJECTION OF ARTIFACTS
# --------------------------------

reject: Optional[dict] = {'grad': 4000e-13, 'mag': 4e-12, 'eeg': 150e-6}
"""
The rejection limits to mark epochs as bads.
This allows to remove strong transient artifacts.
If you want to reject and retrieve blinks or ECG artifacts later, e.g.
with ICA, don't specify a value for the EOG and ECG channels, respectively
(see examples below).

Pass ``None`` to avoid automated epoch rejection based on amplitude.

???+ note "Note"
    These numbers tend to vary between subjects.. You might want to consider
    using the autoreject method by Jas et al. 2018.
    See https://autoreject.github.io


???+ example "Example"
    ```python
    reject = {'grad': 4000e-13, 'mag': 4e-12, 'eog': 150e-6}
    reject = {'grad': 4000e-13, 'mag': 4e-12, 'eeg': 200e-6}
    reject = None
    ```
"""

###############################################################################
# RENAME EXPERIMENTAL EVENTS
# --------------------------

rename_events: dict = dict()
"""
A dictionary specifying which events in the BIDS dataset to rename upon
loading, and before processing begins.

Pass an empty dictionary to not perform any renaming.

???+ example "Example"
    Rename ``audio_left`` in the BIDS dataset to ``audio/left`` in the
    pipeline:
    ```python
    rename_events = {'audio_left': 'audio/left'}
    ```
"""

###############################################################################
# EPOCHING
# --------

conditions: Iterable[str] = ['left', 'right']
"""
The condition names to consider. This can either be name of the
experimental condition as specified in the BIDS ``events.tsv`` file; or
the name of condition *grouped*, if the condition names contain the
(MNE-specific) group separator, ``/``. See the [Subselecting epochs
tutorial](https://mne.tools/stable/auto_tutorials/epochs/plot_10_epochs_overview.html#subselecting-epochs)
for more information.

???+ example "Example"
    ```python
    conditions = ['auditory/left', 'visual/left']
    conditions = ['auditory/left', 'auditory/right']
    conditions = ['auditory']  # All "auditory" conditions (left AND right)
    conditions = ['auditory', 'visual']
    conditions = ['left', 'right']
    ```
"""

tmin = -0.2
"""
The beginning of an epoch, relative to the respective event, in seconds.

???+ example "Example"
    ```python
    tmin = -0.2  # take 200ms before event onset.
    ```
"""

tmax: float = 0.5
"""
The end of an epoch, relative to the respective event, in seconds.
???+ example "Example"
    ```python
    tmax = 0.5  # take 500ms after event onset.
    ```
"""

baseline: Optional[tuple] = (None, 0)
"""
Specifies how to baseline-correct the epochs; if ``None``, no baseline
correction is applied.

???+ example "Example"
    ```python
    baseline = (None, 0)  # baseline between tmin and 0
    ```
"""

contrasts: Iterable[tuple] = []
"""
The conditions to contrast via a subtraction of ERPs / ERFs. Each tuple
in the list corresponds to one contrast. The condition names must be
specified in ``conditions`` above. Pass an empty list to avoid calculation
of contrasts.

???+ example "Example"
    Contrast the "left" and the "right" conditions by calculating
    ``left - right`` at every time point of the evoked responses:
    ```python
    conditions = ['left', 'right']
    contrasts = [('left', 'right')]  # Note we pass a tuple inside the list!
    ```

    Contrast the "left" and the "right" conditions within the "auditory" and
    the "visual" modality, and "auditory" vs "visual" regardless of side:
    ```python
    conditions = ['auditory/left', 'auditory/right',
                  'visual/left', 'visual/right']
    contrasts = [('auditory/left', 'auditory/right'),
                 ('visual/left', 'visual/right'),
                 ('auditory', 'visual')]
    ```
"""
###############################################################################
# ARTIFACT REMOVAL
# ----------------
#
# You can choose between ICA and SSP to remove eye and heart artifacts.
# SSP: https://mne-tools.github.io/stable/auto_tutorials/plot_artifacts_correction_ssp.html?highlight=ssp # noqa
# ICA: https://mne-tools.github.io/stable/auto_tutorials/plot_artifacts_correction_ica.html?highlight=ica # noqa
# if you choose ICA, run scripts 5a and 6a
# if you choose SSP, run scripts 5b and 6b
#
# Currently you cannot use both.

# SSP
# ~~~

use_ssp: bool = True
"""
Whether signal-space projection should be used or not.
"""

# ICA
# ~~~

use_ica: bool = False
"""
Whether independent component analysis should be used or not.
"""

ica_algorithm: Literal['picard', 'fastica', 'extended_infomax']= 'picard'
"""
The ICA algorithm to use.
"""

ica_l_freq: Optional[float] = 1.
"""
The cutoff frequency of the high-pass filter to apply before running ICA.
Using a relatively high cutoff like 1 Hz will remove slow drifts from the
data, yielding improved ICA results.

Set to ``None`` to not apply an additional high-pass filter.

Note: Note
      The filter will be applied to raw data which was already filtered
      according to the ``l_freq`` and ``h_freq`` settings. After filtering, the
      data will be epoched, and the epochs will be submitted to ICA.
"""

ica_max_iterations: int = 200
"""
Maximum number of iterations to decompose the data into independent
components. A low number means to finish earlier, but the consequence is
that the algorithm may not have finished converging. To ensure
convergence, pick a high number here (e.g. 3000); yet the algorithm will
terminate as soon as it determines that is has successfully converged, and
not necessarily exhaust the maximum number of iterations. Note that the
default of 200 seems to be sufficient for Picard in many datasets, because
it converges quicker than the other algorithms; but e.g. for FastICA, this
limit may be too low to achieve convergence.
"""

ica_n_components: Optional[Union[float, int]] = 0.8
"""
MNE conducts ICA as a sort of a two-step procedure: First, a PCA is run
on the data (trying to exclude zero-valued components in rank-deficient
data); and in the second step, the principal componenets are passed
to the actual ICA. You can select how many of the total principal
components to pass to ICA – it can be all or just a subset. This determines
how many independent components to fit, and can be controlled via this
setting.

If int, specifies the number of principal components that are passed to the
ICA algorithm, which will be the number of independent components to
fit. It must not be greater than the rank of your data (which is typically
the number of channels, but may be less in some cases).

If float between 0 and 1, all principal components with cumulative
explained variance less than the value specified here will be passed to
ICA.

If ``None``, **all** principal components will be used.

This setting may drastically alter the time required to compute ICA.
"""

ica_decim: Optional[int] = None
"""
The decimation parameter to compute ICA. If 5 it means
that 1 every 5 sample is used by ICA solver. The higher the faster
it is to run but the less data you have to compute a good ICA. Set to
``1`` or ``None`` to not perform any decimation.
"""

ica_ctps_ecg_threshold: float = 0.1
"""
The threshold parameter passed to `find_bads_ecg` method.
"""

ica_eog_threshold: float = 3.0
"""
The threshold to use during automated EOG classification. Lower values mean
that more ICs will be identified as EOG-related. If too low, the
false-alarm rate increases dramatically.
"""

###############################################################################
# DECODING
# --------

decode: bool = True
"""
Whether to perform decoding (MVPA) on the contrasts specified above as
"contrasts". MVPA will be performed on the level of individual epochs.
"""

decoding_metric: str = 'roc_auc'
"""
The metric to use for cross-validation. It can be `'roc_auc'` or `'accuracy'`
or any other metric supported by `scikit-learn`.

With AUC, chance level is the same regardless of class balance.
"""

decoding_n_splits: int = 5
"""
The number of folds (a.k.a. splits) to use in the cross-validation.
"""

n_boot: int = 5000
"""
The number of bootstrap resamples when estimating the standard error and
confidence interval of the mean decoding score.
"""

###############################################################################
# GROUP AVERAGE SENSORS
# ---------------------

interpolate_bads_grand_average: bool = True
"""
Interpolate bad sensors in each dataset before calculating the grand
average. This parameter is passed to the `mne.grand_average` function via
the keyword argument `interpolate_bads`. It requires to have channel
locations set.

???+ example "Example"
    ```python
    interpolate_bads_grand_average = True
    ```
"""

###############################################################################
# TIME-FREQUENCY
# --------------

time_frequency_conditions: Iterable[str] = []
"""
The conditions to compute time-frequency decomposition on.

???+ example "Example"
    ```python
    time_frequency_conditions = ['left', 'right']
    ```
"""

###############################################################################
# SOURCE SPACE PARAMETERS
# -----------------------
#

spacing: str = 'oct6'
"""
The spacing to use. Can be ``'ico#'`` for a recursively subdivided
icosahedron, ``'oct#'`` for a recursively subdivided octahedron,
``'all'`` for all points, or an integer to use appoximate
distance-based spacing (in mm).
"""

mindist: float = 5
"""
Exclude points closer than this distance (mm) to the bounding surface.
"""

loose: Union[float, Literal['auto']] = 0.2
# ``loose`` : float in [0, 1] | 'auto'
"""
Value that weights the source variances of the dipole components
that are parallel (tangential) to the cortical surface. If loose
is 0 then the solution is computed with fixed orientation,
and fixed must be True or "auto".
If loose is 1, it corresponds to free orientations.
The default value ('auto') is set to 0.2 for surface-oriented source
space and set to 1.0 for volumetric, discrete, or mixed source spaces,
unless ``fixed is True`` in which case the value 0. is used.
"""

depth: Optional[Union[float, dict]] = 0.8
"""
If float (default 0.8), it acts as the depth weighting exponent (``exp``)
to use (must be between 0 and 1). None is equivalent to 0, meaning no
depth weighting is performed. Can also be a `dict` containing additional
keyword arguments to pass to :func:`mne.forward.compute_depth_prior`
(see docstring for details and defaults).
"""

inverse_method: Literal['MNE', 'dSPM', 'sLORETA', 'eLORETA'] = 'dSPM'
"""
Use minimum norm, dSPM (default), sLORETA, or eLORETA to calculate the inverse
solution.
"""

noise_cov: Union[tuple, Literal['emptyroom']] = (None, 0)
"""
Specify how to estimate the noise covariance matrix, which is used in
inverse modeling.

If a tuple, it takes the form ``(tmin, tmax)`` with the time specified in
seconds. If the first value of the tuple is ``None``, the considered
period starts at the beginning of the epoch. If the second value of the
tuple is ``None``, the considered period ends at the end of the epoch.
The default, ``(None, 0)``, includes the entire period before the event,
which is typically the pre-stimulus period.

If ``emptyroom``, the noise covariance matrix will be estimated from an
empty-room MEG recording. The empty-room recording will be automatically
selected based on recording date and time.

Please note that when processing data that contains EEG channels, the noise
covariance can ONLY be estimated from the pre-stimulus period.

???+ example "Example"
    Use the period from start of the epoch until 100 ms before the experimental
    event:
    ```python
    noise_cov = (None, -0.1)
    ```

    Use the time period from the experimental event until the end of the epoch:
    ```python
    noise_cov = (0, None)
    ```

    Use an empty-room recording:
    ```python
    noise_cov = 'emptyroom'
    ```
"""

smooth: Optional[int] = 10
"""
Number of iterations for the smoothing of the surface data.
If None, smooth is automatically defined to fill the surface
with non-zero values. The default is spacing=None.
"""

###############################################################################
# ADVANCED
# --------

l_trans_bandwidth: Union[float, Literal['auto']] = 'auto'
"""
Specifies the transition bandwidth of the
highpass filter. By default it's `'auto'` and uses default MNE
parameters.
"""

h_trans_bandwidth: Union[float, Literal['auto']] = 'auto'
"""
Specifies the transition bandwidth of the
lowpass filter. By default it's `'auto'` and uses default MNE
parameters.
"""

N_JOBS: int = 1
"""
Specifies how many subjects you want to process in parallel.
"""

random_state: Optional[int] = 42
# ``random_state`` : None | int | np.random.RandomState
"""
To specify the seed or state of the random number generator (RNG).
This setting is passed to the ICA algorithm and to the decoding function,
ensuring reproducible results. Set to ``None`` to avoid setting the RNG
to a defined state.
"""

shortest_event: int = 1
"""
Minimum number of samples an event must last. If the
duration is less than this an exception will be raised.
"""

allow_maxshield: bool = False
"""
To import data that was recorded with Maxshield on before running
Maxfilter set this to ``True``.
"""

log_level: Literal['info', 'error'] = 'info'
"""
Set the Study Template logging verbosity.
"""

mne_log_level: Literal['info', 'error'] = 'error'
"""
Set the MNE-Python logging verbosity.
"""

on_error: Literal['continue', 'abort'] = 'abort'
"""
Whether to abort processing as soon as an error occurs, or whether to
continue with all other processing steps for as long as possible.
"""

###############################################################################
#                                                                             #
#                      CUSTOM CONFIGURATION ENDS HERE                         #
#                                                                             #
###############################################################################

###############################################################################
# Name, version, and hosting location of the pipeline
# ---------------------------------------------------

PIPELINE_NAME = 'mne-study-template'
VERSION = '0.1.dev0'
CODE_URL = 'https://github.com/mne-tools/mne-study-template'

###############################################################################
# Logger
# ------

logger = logging.getLogger('mne-study-template')

log_fmt = '%(asctime)s %(message)s'
log_date_fmt = coloredlogs.DEFAULT_DATE_FORMAT = '%H:%M:%S'
coloredlogs.install(level=log_level, logger=logger, fmt=log_fmt,
                    date_fmt=log_date_fmt)

mne.set_log_level(verbose=mne_log_level.upper())

###############################################################################
# Retrieve custom configuration options
# -------------------------------------
#
# For testing a specific dataset, create a Python file with a name of your
# liking (e.g., ``mydataset-template-config.py``), and set an environment
# variable ``MNE_BIDS_STUDY_CONFIG`` to that file.
#
# Example
# ~~~~~~~
# ``export MNE_BIDS_STUDY_CONFIG=/data/mystudy/mydataset-template-config.py``

if "MNE_BIDS_STUDY_CONFIG" in os.environ:
    cfg_path = os.environ['MNE_BIDS_STUDY_CONFIG']

    if os.path.exists(cfg_path):
        msg = f'Using custom configuration: {cfg_path}'
        logger.info(msg)
    else:
        msg = ('The custom configuration file specified in the '
               'MNE_BIDS_STUDY_CONFIG environment variable could not be '
               'found: {cfg_path}'.format(cfg_path=cfg_path))
        raise ValueError(msg)

    # Import configuration from an arbitrary path without having to fiddle
    # with `sys.path`.
    spec = importlib.util.spec_from_file_location(name='custom_config',
                                                  location=cfg_path)
    custom_cfg = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(custom_cfg)
    del spec, cfg_path

    new = None
    for val in dir(custom_cfg):
        if not val.startswith('__'):
            exec("new = custom_cfg.%s" % val)
            logger.debug('Overwriting: %s -> %s' % (val, new))
            exec("%s = custom_cfg.%s" % (val, val))


# BIDS_ROOT environment variable takes precedence over any configuration file
# values.
if os.getenv('BIDS_ROOT') is not None:
    bids_root = os.getenv('BIDS_ROOT')

# If we don't have a bids_root until now, raise an exeception as we cannot
# proceed.
if not bids_root:
    msg = ('You need to specify `bids_root` in your configuration, or '
           'define an environment variable `BIDS_ROOT` pointing to the '
           'root folder of your BIDS dataset')
    raise ValueError(msg)


###############################################################################
# Derivates root
# --------------
deriv_root = os.path.join(bids_root, 'derivatives', PIPELINE_NAME)


###############################################################################
# CHECKS
# ------

if (use_maxwell_filter and
        len(set(ch_types).intersection(('meg', 'grad', 'mag'))) == 0):
    raise ValueError('Cannot use maxwell filter without MEG channels.')

if use_ssp and use_ica:
    raise ValueError('Cannot use both SSP and ICA.')

if use_ica and ica_algorithm not in ('picard', 'fastica', 'extended_infomax'):
    msg = (f"Invalid ICA algorithm requested. Valid values for ica_algorithm "
           f"are: 'picard', 'fastica', and 'extended_infomax', but received "
           f"{ica_algorithm}.")
    raise ValueError(msg)

if use_ica and ica_l_freq < l_freq:
    msg = (f'You requested a lower high-pass filter cutoff frequency for ICA '
           f'than for your raw data: ica_l_freq = {ica_l_freq} < '
           f'l_freq = {l_freq}. Adjust the cutoffs such that ica_l_freq >= '
           f'l_freq, or set ica_l_freq to None if you do not wish to apply '
           f'an additional high-pass filter before running ICA.')
    raise ValueError(msg)

if not ch_types:
    msg = 'Please specify ch_types in your configuration.'
    raise ValueError(msg)

if ch_types == ['eeg']:
    pass
elif 'eeg' in ch_types and len(ch_types) > 1:  # EEG + some other channel types
    msg = ('EEG data can only be analyzed separately from other channel '
           'types. Please adjust `ch_types` in your configuration.')
    raise ValueError(msg)
elif any([ch_type not in ('meg', 'mag', 'grad') for ch_type in ch_types]):
    msg = ('Invalid channel type passed. Please adjust `ch_types` in your '
           'configuration.')
    raise ValueError(msg)

if 'eeg' in ch_types:
    if use_ssp:
        msg = ('You requested SSP for EEG data via use_ssp=True. However, '
               'this is not presently supported. Please use ICA instead by '
               'setting use_ssp=False and use_ica=True.')
        raise ValueError(msg)
    if not use_ica:
        msg = ('You did not request ICA artifact correction for your data. '
               'To turn it on, set use_ica=True.')
        logger.info(msg)

if on_error not in ('continue', 'abort'):
    msg = (f"on_error must be one of 'continue' or 'abort', but received "
           f"{on_error}.")
    logger.info(msg)

if isinstance(noise_cov, str) and noise_cov != 'emptyroom':
    msg = (f"noise_cov must be a tuple or 'emptyroom', but received "
           f"{noise_cov}")
    raise ValueError(msg)

if noise_cov == 'emptyroom' and 'eeg' in ch_types:
    msg = ('You requested to process data that contains EEG channels. In this '
           'case, noise covariance can only be estimated from the '
           'experimental data, e.g., the pre-stimulus period. Please set '
           'noise_cov to (tmin, tmax)')
    raise ValueError(msg)

if noise_cov == 'emptyroom' and not process_er:
    msg = ('You requested noise covariance estimation from empty-room '
           'recordings by setting noise_cov = "emptyroom", but you did not '
           'enable empty-room data processing. Please set process_er = True')
    raise ValueError(msg)


###############################################################################
# Helper functions
# ----------------

def get_sessions():
    sessions_ = copy.deepcopy(sessions)  # Avoid clash with global variable.

    if sessions_ == 'all':
        sessions_ = get_entity_vals(bids_root, entity_key='session')

    if not sessions_:
        return [None]
    else:
        return sessions_


def get_runs():
    runs_ = copy.deepcopy(runs)  # Avoid clash with global variable.

    if runs_ == 'all':
        runs_ = get_entity_vals(bids_root, entity_key='run')

    if not runs_:
        return [None]
    else:
        return runs_


# XXX This check should actually go into the CHECKS section, but it depends
# XXX on get_runs(), which is defined after that section.
if mf_reference_run is not None and mf_reference_run not in get_runs():
    msg = (f'You set mf_reference_run={mf_reference_run}, but your dataset '
           f'only contains the following runs: {get_runs()}')
    raise ValueError(msg)


def get_mf_reference_run():
    # Retrieve to run identifier (number, name) of the reference run
    if mf_reference_run is None:
        # Use the first run
        return get_runs()[0]
    else:
        return mf_reference_run


def get_subjects():
    global subjects

    if subjects == 'all':
        s = get_entity_vals(bids_root, entity_key='subject')
    else:
        s = subjects

    subjects = set(s) - set(exclude_subjects)
    # Drop empty-room subject.
    subjects = subjects - set(['emptyroom'])

    return list(subjects)


def get_task():
    if not task:
        tasks = get_entity_vals(bids_root, entity_key='task')
        if not tasks:
            return None
        else:
            return tasks[0]
    else:
        return task


def get_datatype():
    # Content of ch_types should be sanitized already, so we don't need any
    # extra sanity checks here.
    if data_type is not None:
        return data_type
    elif data_type is None and ch_types == ['eeg']:
        return 'eeg'
    elif data_type is None and any([t in ['meg', 'mag', 'grad']
                                    for t in ch_types]):
        return 'meg'
    else:
        raise RuntimeError("This probably shouldn't happen. Please contact "
                           "the mne-study-template developers. Thank you.")


def get_reject():
    if reject is None:
        return dict()

    reject_ = reject.copy()  # Avoid clash with global variable.

    if ch_types == ['eeg']:
        ch_types_to_remove = ('mag', 'grad')
    else:
        ch_types_to_remove = ('eeg',)

    for ch_type in ch_types_to_remove:
        try:
            del reject_[ch_type]
        except KeyError:
            pass
    return reject_


def get_fs_subjects_dir():
    if not subjects_dir:
        return os.path.join(bids_root, 'derivatives', 'freesurfer', 'subjects')
    else:
        return subjects_dir


def gen_log_message(message, step=None, subject=None, session=None, run=None):
    if subject is not None:
        subject = f'sub-{subject}'
    if session is not None:
        session = f'ses-{session}'
    if run is not None:
        run = f'run-{run}'

    prefix = ', '.join([item for item in [subject, session, run]
                        if item is not None])
    if prefix:
        prefix = f'[{prefix}]'

    if step is not None:
        prefix = f'[Step-{step:02}]{prefix}'

    return prefix + ' ' + message


def failsafe_run(on_error):
    def failsafe_run_decorator(func):
        @functools.wraps(func)  # Preserve "identity" of original function
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                message = 'A critical error occurred.'
                message = gen_log_message(message=message)

                if on_error == 'abort':
                    logger.critical(message)
                    raise(e)
                elif on_error == 'debug':
                    logger.critical(message)
                    extype, value, tb = sys.exc_info()
                    traceback.print_exc()
                    pdb.post_mortem(tb)
                else:
                    message = f'{message} The error message was:\n{str(e)}'
                    logger.critical(message)
        return wrapper
    return failsafe_run_decorator


def plot_auto_scores(auto_scores):
    # Plot scores of automated bad channel detection.
    import matplotlib.pyplot as plt
    import seaborn as sns
    import pandas as pd

    if ch_types == ['meg']:
        ch_types_ = ['grad', 'mag']
    else:
        ch_types_ = ch_types

    figs = []
    for ch_type in ch_types_:
        # Only select the data for mag or grad channels.
        ch_subset = auto_scores['ch_types'] == ch_type
        ch_names = auto_scores['ch_names'][ch_subset]
        scores = auto_scores['scores_noisy'][ch_subset]
        limits = auto_scores['limits_noisy'][ch_subset]
        bins = auto_scores['bins']  # The the windows that were evaluated.

        # We will label each segment by its start and stop time, with up to 3
        # digits before and 3 digits after the decimal place (1 ms precision).
        bin_labels = [f'{start:3.3f} – {stop:3.3f}'
                      for start, stop in bins]

        # We store the data in a Pandas DataFrame. The seaborn heatmap function
        # we will call below will then be able to automatically assign the
        # correct labels to all axes.
        data_to_plot = pd.DataFrame(data=scores,
                                    columns=pd.Index(bin_labels,
                                                     name='Time (s)'),
                                    index=pd.Index(ch_names, name='Channel'))

        # First, plot the "raw" scores.
        fig, ax = plt.subplots(1, 2, figsize=(12, 8))
        fig.suptitle(f'Automated noisy channel detection: {ch_type}',
                     fontsize=16, fontweight='bold')
        sns.heatmap(data=data_to_plot, cmap='Reds',
                    cbar_kws=dict(label='Score'), ax=ax[0])
        [ax[0].axvline(x, ls='dashed', lw=0.25, dashes=(25, 15), color='gray')
            for x in range(1, len(bins))]
        ax[0].set_title('All Scores', fontweight='bold')

        # Now, adjust the color range to highlight segments that exceeded the
        # limit.
        sns.heatmap(data=data_to_plot,
                    vmin=np.nanmin(limits),  # input data may contain NaNs
                    cmap='Reds', cbar_kws=dict(label='Score'), ax=ax[1])
        [ax[1].axvline(x, ls='dashed', lw=0.25, dashes=(25, 15), color='gray')
            for x in range(1, len(bins))]
        ax[1].set_title('Scores > Limit', fontweight='bold')

        # The figure title should not overlap with the subplots.
        fig.tight_layout(rect=[0, 0.03, 1, 0.95])
        figs.append(fig)

    return figs


def get_channels_to_analyze(info):
    # Return names of the channels of the channel types we wish to analyze.
    # We also include channels marked as "bad" here.
    # `exclude=[]`: keep "bad" channels, too.
    if get_datatype() == 'meg' and ('mag' in ch_types or 'grad' in ch_types
                                    or 'meg' in ch_types):
        pick_idx = mne.pick_types(info, eog=True, ecg=True, exclude=[])

        if 'mag' in ch_types:
            pick_idx += mne.pick_types(info, meg='mag', exclude=[])
        if 'grad' in ch_types:
            pick_idx += mne.pick_types(info, meg='grad', exclude=[])
        if 'meg' in ch_types:
            pick_idx = mne.pick_types(info, meg=True, eog=True, ecg=True,
                                      exclude=[])
    elif ch_types == ['eeg']:
        pick_idx = mne.pick_types(info, meg=False, eeg=True, eog=True,
                                  ecg=True, exclude=[])
    else:
        raise RuntimeError('Something unexpected happened. Please contact '
                           'the mne-study-template developers. Thank you.')

    ch_names = [info['ch_names'][i] for i in pick_idx]
    return ch_names
