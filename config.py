"""Default settings for data processing and analysis. Do not edit this file,
but instead create a new configuration changing only the settings you need to
alter for your specific analysis.
"""
import importlib
import pathlib
import functools
import os
import pdb
import traceback
import sys
import copy
import logging

from typing import Optional, Union, Iterable, List, Tuple, Dict, Callable
if sys.version_info >= (3, 8):
    from typing import Literal
else:
    from typing_extensions import Literal

import coloredlogs
import numpy as np
import mne
from mne_bids.path import get_entity_vals

PathLike = Union[str, pathlib.Path]


study_name: str = ''
"""
Specify the name of your study. It will be used to populate filenames for
saving the analysis results.

???+ example "Example"
    ```python

    study_name = 'my-study'
    ```

{{ general.md }}

"""

bids_root: Optional[PathLike] = None
"""
Specify the BIDS root directory. Pass an empty string or ```None`` to use
the value specified in the ``BIDS_ROOT`` environment variable instead.
Raises an exception if the BIDS root has not been specified.

???+ example "Example"
    ``` python
    bids_root = '/path/to/your/bids_root'  # Use this to specify a path here.
    bids_root = None  # Make use of the ``BIDS_ROOT`` environment variable.
    ```

{{ general.md }}

"""

deriv_root: Optional[PathLike] = None
"""
The root of the derivatives directory in which the pipeline will store
the processing results. If ``None``, this will be
``derivatives/mne-bids-pipeline`` inside the BIDS root.

{{ general.md }}

"""

subjects_dir: Optional[PathLike] = None
"""
Path to the directory that contains the MRI data files and their
derivativesfor all subjects. Specifically, the ``subjects_dir`` is the
$SUBJECTS_DIR used by the Freesurfer software.

{{ general.md }}

"""

interactive: bool = False
"""
If True, the scripts will provide some interactive elements, such as
figures. If running the scripts from a notebook or Spyder,
run `%matplotlib qt` in the command line to open the figures in a separate
window.

{{ preprocessing, sensor, source }}

"""

crop: Optional[Tuple[float, float]] = None
"""
Crop the raw data to the specified time interval ``[tmin, tmax]`` in seconds.
If ``None``, do not crop the data.

{{ preprocessing }}

"""

sessions: Union[List, Literal['all']] = 'all'
"""
The sessions to process.

{{ general.md }}

"""

task: str = ''
"""
The task to process.

{{ general.md }}

"""

runs: Union[Iterable, Literal['all']] = 'all'
"""
The runs to process.

{{ general.md }}

"""

acq: Optional[str] = None
"""
The BIDS `acquisition` entity.

{{ preprocessing, sensor, source, report }}

"""

proc: Optional[str] = None
"""
The BIDS `processing` entity.

{{ preprocessing, sensor, source, report }}

"""

rec: Optional[str] = None
"""
The BIDS `recording` entity.

{{ preprocessing, sensor, source, report }}

"""

space: Optional[str] = None
"""
The BIDS `space` entity.

{{ preprocessing, sensor, source, report }}

"""

subjects: Union[Iterable[str], Literal['all']] = 'all'
"""
Subjects to analyze. If ``'all'``, include all subjects. To only
include a subset of subjects, pass a list of their identifiers. Even
if you plan on analyzing only a single subject, pass their identifier
as a list.

Please note that if you intend to EXCLUDE only a few subjects, you
should consider setting ``subjects = 'all'`` and adding the
identifiers of the excluded subjects to ``exclude_subjects`` (see next
section).

???+ example "Example"
    ```python
    subjects = 'all'  # Include all subjects.
    subjects = ['05']  # Only include subject 05.
    subjects = ['01', '02']  # Only include subjects 01 and 02.
    ```

{{ general.md }}

"""

exclude_subjects: Iterable[str] = []
"""
Specify subjects to exclude from analysis. The MEG empty-room mock-subject
is automatically excluded from regular analysis.

???+ info "Good Practice / Advice"
    Keep track of the criteria leading you to exclude
    a participant (e.g. too many movements, missing blocks, aborted experiment,
    did not understand the instructions, etc, ...)
    The ``emptyroom`` subject will be excluded automatically.

{{ general.md }}

"""

process_er: bool = False
"""
Whether to apply the same pre-processing steps to the empty-room data as
to the experimental data (up until including frequency filtering). This
is required if you wish to use the empty-room recording to estimate noise
covariance (via ``noise_cov='emptyroom'``). The empty-room recording
corresponding to the processed experimental data will be retrieved
automatically.

{{ preprocessing, report }}

"""

ch_types: Iterable[Literal['meg', 'mag', 'grad', 'eeg']] = []
"""
The channel types to consider.

!!! info
    Currently, MEG and EEG data cannot be processed together.

???+ example "Example"
    ```python
    # Use EEG channels:
    ch_types = ['eeg']

    # Use magnetometer and gradiometer MEG channels:
    ch_types = ['mag', 'grad']

    # Currently does not work and will raise an error message:
    ch_types = ['meg', 'eeg']
    ```
{{ preprocessing, sensor, source }}

"""

data_type: Optional[Literal['meg', 'eeg']] = None
"""
The BIDS data type.

For MEG recordings, this will usually be 'meg'; and for EEG, 'eeg'.
However, if your dataset contains simultaneous recordings of MEG and EEG,
stored in a single file, you will typically need to set this to 'meg'.
If ``None``, we will assume that the data type matches the channel type.

???+ example "Example"
    The dataset contains simultaneous recordings of MEG and EEG, and we only
    wish to process the EEG data, which is stored inside the MEG files:

    ```python
    ch_types = ['eeg']
    data_type = 'eeg'
    ```

    The dataset contains simultaneous recordings of MEG and EEG, and we only
    wish to process the gradiometer data:

    ```python
    ch_types = ['grad']
    data_type = 'meg'  # or data_type = None
    ```

    The dataset contains only EEG data:

    ```python
    ch_types = ['eeg']
    data_type = 'eeg'  # or data_type = None
    ```

{{ general.md }}

"""

eog_channels: Optional[Iterable[str]] = None
"""
Specify EOG channels to use, or create virtual EOG channels.

Allows the specification of custom channel names that shall be used as
(virtual) EOG channels. For example, say you recorded EEG **without** dedicated
EOG electrodes, but with some EEG electrodes placed close to the eyes, e.g.
Fp1 and Fp2. These channels can be expected to have captured large quantities
of ocular activity, and you might want to use them as "virtual" EOG channels,
while also including them in the EEG analysis. By default, MNE won't know that
these channels are suitable for recovering EOG, and hence won't be able to
perform tasks like automated blink removal, unless a "true" EOG sensor is
present in the data as well. Specifying channel names here allows MNE to find
the respective EOG signals based on these channels.

You can specify one or multiple channel names. Each will be treated as if it
were a dedicated EOG channel, without excluding it from any other analyses.

If ``None``, only actual EOG channels will be used for EOG recovery.

If there are multiple actual EOG channels in your data, and you only specify
a subset of them here, only this subset will be used during processing.

???+ example "Example"
    Treat ``Fp1`` as virtual EOG channel:
    ```python
    eog_channels = ['Fp1']
    ```

    Treat ``Fp1`` and ``Fp2`` as virtual EOG channels:
    ```python
    eog_channels = ['Fp1', 'Fp2']
    ```

{{ preprocessing }}

"""

eeg_bipolar_channels: Optional[Dict[str, Tuple[str, str]]] = None
"""
Combine two channels into a bipolar channel, whose signal is the **difference**
between the two combined channels, and add it to the data.
A typical use case is the combination of two EOG channels – for example, a
left and a right horizontal EOG – into a single, bipolar EOG channel. You need
to pass a dictionary whose **keys** are the name of the new bipolar channel you
wish to create, and whose **values** are tuples consisting of two strings: the
name of the channel acting as anode and the name of the channel acting as
cathode, i.e. `{'ch_name': ('anode', 'cathode')}`. You can request
to construct more than one bipolar channel by specifying multiple key/value
pairs. See the examples below.

Can also be `None` if you do not want to create bipolar channels.

Note: Note
    The channels used to create the bipolar channels are **not** automatically
    dropped from the data. To drop channels, set `drop_channels`.

???+ example "Example"
    Combine the existing channels `HEOG_left` and `HEOG_right` into a new,
    bipolar channel, `HEOG`:
    ```python
    eeg_add_bipolar_channels = {'HEOG': ('HEOG_left', 'HEOG_right')}
    ```

    Create two bipolar channels, `HEOG` and `VEOG`:
    ```python
    eeg_add_bipolar_channels = {'HEOG': ('HEOG_left', 'HEOG_right'),
                                'VEOG': ('VEOG_lower', 'VEOG_upper')}
    ```
{{ preprocessing }}

"""

eeg_reference: Union[Literal['average'], str, Iterable['str']] = 'average'
"""
The EEG reference to use. If ``average``, will use the average reference,
i.e. the average across all channels. If a string, must be the name of a single
channel. To use multiple channels as reference, set to a list of channel names.

???+ example "Example"
    Use the average reference:
    ```python
    eeg_reference = 'average'
    ```

    Use the `P9` channel as reference:
    ```python
    eeg_reference = 'P9'
    ```

    Use the average of the `P9` and `P10` channels as reference:
    ```python
    eeg_reference = ['P9', 'P10']
    ```
{{ preprocessing, sensor }}

"""

eeg_template_montage: Optional[str] = None
"""
In situations where you wish to process EEG data and no individual
digitization points (measured channel locations) are available, you can apply
a "template" montage. This means we will assume the EEG cap was placed
either according to an international system like 10/20, or as suggested by
the cap manufacturers in their respective manual.

Please be aware that the actual cap placement most likely deviated somewhat
from the template, and, therefore, source reconstruction may be impaired.

If ``None``, do not apply a template montage. If a string, must be the
name of a built-in template montage in MNE-Python.
You can find an overview of supported template montages at
https://mne.tools/stable/generated/mne.channels.make_standard_montage.html

???+ example "Example"
    Do not apply template montage:
    ```python
    eeg_template_montage = None
    ```

    Apply 64-channel Biosemi 10/20 template montage:
    ```python
    eeg_template_montage = 'biosemi64'
    ```
{{ preprocessing }}

"""

drop_channels: Iterable[str] = []
"""
Names of channels to remove from the data. This can be useful, for example,
if you have added a new bipolar channel via `eeg_bipolar_channels` and now wish
to remove the anode, cathode, or both.

???+ example "Example"
    Exclude channels `Fp1` and `Cz` from processing:
    ```python
    drop_channels = ['Fp1', 'Cz]
    ```
{{ preprocessing }}

"""

analyze_channels: Union[Literal['all'], Iterable['str']] = 'all'
"""
The names of the channels to analyze during ERP/ERF and time-frequency analysis
steps. For certain paradigms, e.g. EEG ERP research, it is common to contrain
sensor-space analysis to only a few specific sensors. If `'all'`, do not
exclude any channels (except for those selected for removal via the
`drop_channels` setting). The constraint will be applied to all sensor-level
analyses after the preprocessing stage, but not to the preprocessing stage
itself, nor to the source analysis stage.

???+ example "Example"
    Only use channel `Pz` for ERP, evoked contrasts, time-by-time
    decoding, and time-frequency analysis:
    ```python
    analyze_channels = ['Pz']
    ```

{{ sensor, report }}

"""

###############################################################################
# MAXWELL FILTER PARAMETERS
# -------------------------
# done in 01-import_and_maxfilter.py

find_flat_channels_meg: bool = False
"""
Auto-detect "flat" channels (i.e. those with unusually low variability) and
mark them as bad.

{{ preprocessing }}

"""

find_noisy_channels_meg: bool = False
"""
Auto-detect "noisy" channels and mark them as bad.

{{ preprocessing, report }}

"""

use_maxwell_filter: bool = False
"""
Whether or not to use Maxwell filtering to preprocess the data.

warning:
    If the data were recorded with internal active compensation (MaxShield),
    they need to be run through Maxwell filter to avoid distortions.
    Bad channels need to be set through BIDS channels.tsv and / or via the
    ``find_flat_channels_meg`` and ``find_noisy_channels_meg`` options above
    before applying Maxwell filter.

{{ preprocessing }}

"""

mf_st_duration: Optional[float] = None
"""
There are two kinds of maxfiltering: SSS (signal space separation) and tSSS
(temporal signal space separation)
(see [Taulu et al., 2004](http://cds.cern.ch/record/709081/files/0401166.pdf)).

If not None, apply spatiotemporal SSS (tSSS) with specified buffer
duration (in seconds). MaxFilter™'s default is 10.0 seconds in v2.2.
Spatiotemporal SSS acts as implicitly as a high-pass filter where the
cut-off frequency is 1/st_dur Hz. For this (and other) reasons, longer
buffers are generally better as long as your system can handle the
higher memory usage. To ensure that each window is processed
identically, choose a buffer length that divides evenly into your data.
Any data at the trailing edge that doesn't fit evenly into a whole
buffer window will be lumped into the previous buffer.

???+ info "Good Practice / Advice"
    If you are interested in low frequency activity (<0.1Hz), avoid using
    tSSS and set ``mf_st_duration`` to ``None``.

    If you are interested in low frequency above 0.1 Hz, you can use the
    default ``mf_st_duration`` to 10 s, meaning it acts like a 0.1 Hz
    high-pass filter.

???+ example "Example"
    ```python
    mf_st_duration = None
    mf_st_duration = 10.  # to apply tSSS with 0.1Hz highpass filter.
    ```

{{ preprocessing }}

"""

mf_head_origin = 'auto'
"""
``mf_head_origin`` : array-like, shape (3,) | 'auto'
Origin of internal and external multipolar moment space in meters.
If 'auto', it will be estimated from headshape points.
If automatic fitting fails (e.g., due to having too few digitization
points), consider separately calling the fitting function with different
options or specifying the origin manually.

???+ example "Example"
    ```python
    mf_head_origin = 'auto'
    ```
{{ preprocessing }}

"""

mf_reference_run: Optional[str] = None
"""
Despite all possible care to avoid movements in the MEG, the participant
will likely slowly drift down from the Dewar or slightly shift the head
around in the course of the recording session. Hence, to take this into
account, we are realigning all data to a single position. For this, you need
to define a reference run (typically the one in the middle of
the recording session).

Which run to take as the reference for adjusting the head position of all
runs. If ``None``, pick the first run.

???+ example "Example"
    ```python
    mf_reference_run = '01'  # Use run "01"
    ```
{{ preprocessing }}

"""

mf_cal_fname: Optional[str] = None
"""
warning:
     This parameter should only be used for BIDS datasets that don't store
     the fine-calibration file
     [according to BIDS](https://bids-specification.readthedocs.io/en/stable/99-appendices/06-meg-file-formats.html#cross-talk-and-fine-calibration-files).
Path to the Maxwell Filter calibration file. If None the recommended
location is used.
???+ example "Example"
    ```python
    mf_cal_fname = '/path/to/your/file/calibration_cal.dat'
    ```
{{ preprocessing }}

"""

mf_ctc_fname: Optional[str] = None
"""
Path to the Maxwell Filter cross-talk file. If None the recommended
location is used.
warning:
     This parameter should only be used for BIDS datasets that don't store
     the cross-talk file
     [according to BIDS](https://bids-specification.readthedocs.io/en/stable/99-appendices/06-meg-file-formats.html#cross-talk-and-fine-calibration-files).
???+ example "Example"
    ```python
    mf_ctc_fname = '/path/to/your/file/crosstalk_ct.fif'
    ```
{{ preprocessing }}

"""

###############################################################################
# STIMULATION ARTIFACT
# --------------------
# used in 01-import_and_maxfilter.py

fix_stim_artifact: bool = False
"""
Apply interpolation to fix stimulation artifact.

???+ example "Example"
    ```python
    fix_stim_artifact = False
    ```
{{ preprocessing }}

"""

stim_artifact_tmin: float = 0.
"""
Start time of the interpolation window in seconds.

???+ example "Example"
    ```python
    stim_artifact_tmin = 0.  # on stim onset
    ```
{{ preprocessing }}

"""

stim_artifact_tmax: float = 0.01
"""
End time of the interpolation window in seconds.

???+ example "Example"
    ```python
    stim_artifact_tmax = 0.01  # up to 10ms post-stimulation
    ```
{{ preprocessing }}

"""

###############################################################################
# FREQUENCY FILTERING
# -------------------
# done in 02-frequency_filter.py

l_freq: Optional[float] = None
"""
The low-frequency cut-off in the highpass filtering step.
Keep it None if no highpass filtering should be applied.

{{ preprocessing }}

"""

h_freq: Optional[float] = 40.
"""
The high-frequency cut-off in the lowpass filtering step.
Keep it None if no lowpass filtering should be applied.

{{ preprocessing, report }}

"""

###############################################################################
# RESAMPLING
# ----------

resample_sfreq: Optional[float] = None
"""
Specifies at which sampling frequency the data should be resampled.
If None then no resampling will be done.

???+ example "Example"
    ```python
    resample_sfreq = None  # no resampling
    resample_sfreq = 500  # resample to 500Hz
    ```
{{ preprocessing }}

"""

decim: int = 1
"""
Says how much to decimate data at the epochs level.
It is typically an alternative to the `resample_sfreq` parameter that
can be used for resampling raw data. ``1`` means no decimation.

???+ info "Good Practice / Advice"
    Decimation requires to lowpass filtered the data to avoid aliasing.
    Note that using decimation is much faster than resampling.

???+ example "Example"
    ```python
    decim = 1  # no decimation
    decim = 4  # decimate by 4 ie devide sampling frequency by 4
    ```
{{ preprocessing }}

"""


###############################################################################
# RENAME EXPERIMENTAL EVENTS
# --------------------------

rename_events: dict = dict()
"""
A dictionary specifying which events in the BIDS dataset to rename upon
loading, and before processing begins.

Pass an empty dictionary to not perform any renaming.

???+ example "Example"
    Rename ``audio_left`` in the BIDS dataset to ``audio/left`` in the
    pipeline:
    ```python
    rename_events = {'audio_left': 'audio/left'}
    ```
{{ preprocessing }}

"""

on_rename_missing_events: Literal['warn', 'raise'] = 'raise'
"""
How to handle the situation where you specified an event to be renamed via
``rename_events``, but this particular event is not present in the data. By
default, we will raise an exception to avoid accidental mistakes due to typos;
however, if you're sure what you're doing, you may change this to ``'warn'``
to only get a warning instead.

{{ preprocessing }}

"""

###############################################################################
# HANDLING OF REPEATED EVENTS
# ---------------------------

event_repeated: Literal['error', 'drop', 'merge'] = 'error'
"""
How to handle repeated events. We call events "repeated" if more than one event
occurred at the exact same time point. Currently, MNE-Python cannot handle
this situation gracefully when trying to create epochs, and will throw an
error. To only keep the event of that time point ("first" here referring to
the order that events appear in `*_events.tsv`), pass `'drop'`. You can also
request to create a new type of event by merging repeated events by setting
this to `'merge'`.

warning:
    The `'merge'` option is entirely untested in the MNE BIDS Pipeline as of
    April 1st, 2021.

{{ preprocessing }}

"""

###############################################################################
# EPOCHING
# --------

epochs_metadata_tmin: Optional[float] = None
"""
The beginning of the time window for metadata generation, in seconds,
relative to the time-locked event of the respective epoch. This may be less
than or larger than the epoch's first time point. If ``None``, use the first
time point of the epoch.

{{ preprocessing }}

"""

epochs_metadata_tmax: Optional[float] = None
"""
Same as ``epochs_metadata_tmin``, but specifying the **end** of the time
window for metadata generation.

{{ preprocessing }}

"""

epochs_metadata_keep_first: Optional[Iterable[str]] = None
"""
Event groupings using hierarchical event descriptors (HEDs) for which to store
the time of the **first** occurrence of any event of this group in a new column
with the group name, and the **type** of that event in a column named after the
group, but with a ``first_`` prefix. If ``None`` (default), no event
aggregation will take place and no new columns will be created.

???+ example "Example"
    Assume you have two response events types, ``response/left`` and
    ``response/right``; in some trials, both responses occur, because the
    participant pressed both buttons. Now, you want to keep the first response
    only. To achieve this, set
    ```python
    epochs_metadata_keep_first = ['response']
    ```
    This will add two new columns to the metadata: ``response``, indicating
    the **time** relative to the time-locked event; and ``first_response``,
    depicting the **type** of event (``'left'`` or ``'right'``).

    You may also specify a grouping for multiple event types:
    ```python
    epochs_metadata_keep_first = ['response', 'stimulus']
    ```
    This will add the columns ``response``, ``first_response``, ``stimulus``,
    and ``first_stimulus``.

{{ preprocessing }}

"""

epochs_metadata_keep_last: Optional[Iterable[str]] = None
"""
Same as ``epochs_metadata_keep_first``, but for keeping the **last**
occurrence of matching event types. The columns indicating the event types
will be named with a ``last_`` instead of a ``first_`` prefix.

{{ preprocessing }}

"""

conditions: Optional[Union[Iterable[str], Dict[str, str]]] = None
"""
The time-locked events based on which to create evoked responses.
This can either be name of the experimental condition as specified in the
BIDS ``*_events.tsv`` file; or the name of condition *groups*, if the condition
names contain the (MNE-specific) group separator, ``/``. See the [Subselecting
epochs tutorial](https://mne.tools/stable/auto_tutorials/epochs/plot_10_epochs_overview.html#subselecting-epochs)
for more information.

Passing a dictionary allows to assign a name to map a complex condition name
(value) to a more legible one (value).

This is a **required** parameter in the configuration file. If left as `None`,
it will raise an error.

???+ example "Example"
    Specifying conditions as lists of strings:
    ```python
    conditions = ['auditory/left', 'visual/left']
    conditions = ['auditory/left', 'auditory/right']
    conditions = ['auditory']  # All "auditory" conditions (left AND right)
    conditions = ['auditory', 'visual']
    conditions = ['left', 'right']
    ```
    Pass a dictionary to define a mapping:
    ```python
    conditions = {'simple_name': 'complex/condition/with_subconditions'}
    conditions = {'correct': 'response/correct',
                  'incorrect': 'response/incorrect'}

{{ preprocessing, sensor, source, report }}

"""

epochs_tmin: float = -0.2
"""
The beginning of an epoch, relative to the respective event, in seconds.

???+ example "Example"
    ```python
    epochs_tmin = -0.2  # 200 ms before event onset
    ```
{{ preprocessing }}

"""

epochs_tmax: float = 0.5
"""
The end of an epoch, relative to the respective event, in seconds.
???+ example "Example"
    ```python
    epochs_tmax = 0.5  # 500 ms after event onset
    ```
{{ preprocessing }}

"""

baseline: Optional[Tuple[Optional[float], Optional[float]]] = (None, 0)
"""
Specifies which time interval to use for baseline correction of epochs;
if ``None``, no baseline correction is applied.

???+ example "Example"
    ```python
    baseline = (None, 0)  # beginning of epoch until time point zero
    ```
{{ preprocessing }}

"""

contrasts: Iterable[Tuple[str, str]] = []
"""
The conditions to contrast via a subtraction of ERPs / ERFs. Each tuple
in the list corresponds to one contrast. The condition names must be
specified in ``conditions`` above. Pass an empty list to avoid calculation
of contrasts.

???+ example "Example"
    Contrast the "left" and the "right" conditions by calculating
    ``left - right`` at every time point of the evoked responses:
    ```python
    conditions = ['left', 'right']
    contrasts = [('left', 'right')]  # Note we pass a tuple inside the list!
    ```

    Contrast the "left" and the "right" conditions within the "auditory" and
    the "visual" modality, and "auditory" vs "visual" regardless of side:
    ```python
    conditions = ['auditory/left', 'auditory/right',
                  'visual/left', 'visual/right']
    contrasts = [('auditory/left', 'auditory/right'),
                 ('visual/left', 'visual/right'),
                 ('auditory', 'visual')]
    ```
{{ sensor, report }}

"""

###############################################################################
# ARTIFACT REMOVAL
# ----------------
#
# You can choose between ICA and SSP to remove eye and heart artifacts.
# SSP: https://mne-tools.github.io/stable/auto_tutorials/plot_artifacts_correction_ssp.html?highlight=ssp # noqa
# ICA: https://mne-tools.github.io/stable/auto_tutorials/plot_artifacts_correction_ica.html?highlight=ica # noqa
# if you choose ICA, run scripts 5a and 6a
# if you choose SSP, run scripts 5b and 6b
#
# Currently you cannot use both.

spatial_filter: Optional[Literal['ssp', 'ica']] = None
"""
Whether to use a spatial filter to detect and remove artifacts. The BIDS
Pipeline offers the use of signal-space projection (SSP) and independent
component analysis (ICA).

Use `'ssp'` for SSP, `'ica'` for ICA, and `None` if you do not wish to apply
a spatial filter for artifact removal.

The Pipeline will try to automatically discover EOG and ECG artifacts. For SSP,
it will then produce projection vectors that remove ("project out") these
artifacts from the data. For ICA, the independent components related to
EOG and ECG activity will be omitted during the signal reconstruction step in
order to remove the artifacts. The ICA procedure can be configured in various
ways using the configuration options you can find below.

{{ preprocessing, sensor, source, report }}

"""

ica_reject: Optional[Dict[str, float]] = None
"""
Peak-to-peak amplitude limits to exclude epochs from ICA fitting.

This allows you to remove strong transient artifacts, which could negatively
affect ICA performance.

The BIDS Pipeline will automatically try to detect EOG and ECG artifacts in
your data, and remove them. For this to work properly, it is recommended
to **not** specify rejection thresholds for EOG and ECG channels here –
otherwise, ICA won't be able to "see" these artifacts.

???+ example "Example"
    ```python
    ica_reject = {'grad': 10e-10, 'mag': 20e-12, 'eeg': 400e-6}
    ica_reject = {'grad': 15e-10}
    ica_reject = None
    ```
{{ preprocessing }}

"""

ica_algorithm: Literal['picard', 'fastica', 'extended_infomax'] = 'picard'
"""
The ICA algorithm to use.

{{ preprocessing }}

"""

ica_l_freq: Optional[float] = 1.
"""
The cutoff frequency of the high-pass filter to apply before running ICA.
Using a relatively high cutoff like 1 Hz will remove slow drifts from the
data, yielding improved ICA results. Must be set to 1 Hz or above.

Set to ``None`` to not apply an additional high-pass filter.

Note: Note
      The filter will be applied to raw data which was already filtered
      according to the ``l_freq`` and ``h_freq`` settings. After filtering, the
      data will be epoched, and the epochs will be submitted to ICA.

!!! info
    The Pipeline will only allow you to perform ICA on data that has been
    high-pass filtered with a 1 Hz cutoff or higher. This is a conscious,
    opinionated (but partially data-driven) decision made by the developers.
    If you have reason to challenge this behavior, please get in touch with
    us so we can discuss.

{{ preprocessing }}

"""

ica_max_iterations: int = 500
"""
Maximum number of iterations to decompose the data into independent
components. A low number means to finish earlier, but the consequence is
that the algorithm may not have finished converging. To ensure
convergence, pick a high number here (e.g. 3000); yet the algorithm will
terminate as soon as it determines that is has successfully converged, and
not necessarily exhaust the maximum number of iterations. Note that the
default of 200 seems to be sufficient for Picard in many datasets, because
it converges quicker than the other algorithms; but e.g. for FastICA, this
limit may be too low to achieve convergence.

{{ preprocessing }}

"""

ica_n_components: Optional[Union[float, int]] = 0.8
"""
MNE conducts ICA as a sort of a two-step procedure: First, a PCA is run
on the data (trying to exclude zero-valued components in rank-deficient
data); and in the second step, the principal componenets are passed
to the actual ICA. You can select how many of the total principal
components to pass to ICA – it can be all or just a subset. This determines
how many independent components to fit, and can be controlled via this
setting.

If int, specifies the number of principal components that are passed to the
ICA algorithm, which will be the number of independent components to
fit. It must not be greater than the rank of your data (which is typically
the number of channels, but may be less in some cases).

If float between 0 and 1, all principal components with cumulative
explained variance less than the value specified here will be passed to
ICA.

If ``None``, **all** principal components will be used.

This setting may drastically alter the time required to compute ICA.

{{ preprocessing }}

"""

ica_decim: Optional[int] = None
"""
The decimation parameter to compute ICA. If 5 it means
that 1 every 5 sample is used by ICA solver. The higher the faster
it is to run but the less data you have to compute a good ICA. Set to
``1`` or ``None`` to not perform any decimation.

{{ preprocessing }}

"""

ica_ctps_ecg_threshold: float = 0.1
"""
The threshold parameter passed to `find_bads_ecg` method.

{{ preprocessing }}

"""

ica_eog_threshold: float = 3.0
"""
The threshold to use during automated EOG classification. Lower values mean
that more ICs will be identified as EOG-related. If too low, the
false-alarm rate increases dramatically.

{{ preprocessing }}

"""


# Rejection based on peak-to-peak amplitude
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

reject: Optional[Dict[str, float]] = None
"""
Peak-to-peak amplitude limits to mark epochs as bad. This allows you to remove
epochs with strong transient artifacts.

Note: Note
      The rejection is performed **after** SSP or ICA, if any of those methods
      is used. To reject epochs before fitting ICA, see the
      [`ica_reject`][config.ica_reject] setting.

Pass ``None`` to avoid automated epoch rejection based on amplitude.

???+ example "Example"
    ```python
    reject = {'grad': 4000e-13, 'mag': 4e-12, 'eog': 150e-6}
    reject = {'eeg': 100e-6, 'eog': 250e-6}
    reject = None
    ```
{{ preprocessing }}

"""

reject_tmin: Optional[float] = None
"""
Start of the time window used to reject epochs. If ``None``, the window will
start with the first time point.
???+ example "Example"
    ```python
    reject_tmin = -0.1  # 100 ms before event onset.
    ```
{{ preprocessing }}

"""

reject_tmax: Optional[float] = None
"""
End of the time window used to reject epochs. If ``None``, the window will end
with the last time point.
???+ example "Example"
    ```python
    reject_tmax = 0.3  # 300 ms after event onset.
    ```
{{ preprocessing }}

"""

###############################################################################
# DECODING
# --------

decode: bool = True
"""
Whether to perform decoding (MVPA) on the contrasts specified above as
"contrasts". MVPA will be performed on the level of individual epochs.

{{ sensor, report }}

"""

decoding_metric: str = 'roc_auc'
"""
The metric to use for cross-validation. It can be `'roc_auc'` or `'accuracy'`
or any other metric supported by `scikit-learn`.

With AUC, chance level is the same regardless of class balance.

{{ sensor, report }}

"""

decoding_n_splits: int = 5
"""
The number of folds (a.k.a. splits) to use in the cross-validation.

{{ sensor }}

"""

n_boot: int = 5000
"""
The number of bootstrap resamples when estimating the standard error and
confidence interval of the mean decoding score.

{{ sensor, report }}

"""

###############################################################################
# GROUP AVERAGE SENSORS
# ---------------------

interpolate_bads_grand_average: bool = True
"""
Interpolate bad sensors in each dataset before calculating the grand
average. This parameter is passed to the `mne.grand_average` function via
the keyword argument `interpolate_bads`. It requires to have channel
locations set.

???+ example "Example"
    ```python
    interpolate_bads_grand_average = True
    ```
{{ sensor }}

"""

###############################################################################
# TIME-FREQUENCY
# --------------

time_frequency_conditions: Iterable[str] = []
"""
The conditions to compute time-frequency decomposition on.

???+ example "Example"
    ```python
    time_frequency_conditions = ['left', 'right']
    ```
{{ sensor }}

"""

###############################################################################
# SOURCE ESTIMATION PARAMETERS
# ----------------------------
#

run_source_estimation: bool = True
"""
Whether to run source estimation processing steps if not explicitly requested.

{{ source }}
"""

bem_mri_images: Literal['FLASH', 'T1', 'auto'] = 'auto'
"""
Which types of MRI images to use when creating the BEM model.
If ``'FLASH'``, use FLASH MRI images, and raise an exception if they cannot be
found.

???+ info "Advice"
    It is recommended to use the FLASH images if available, as the quality
    of the extracted BEM surfaces will be higher.

If ``'T1'``, create the BEM surfaces from the T1-weighted images using the
``watershed`` algorithm.

If ``'auto'``, use FLASH images if available, and use the ``watershed``
algorithm with the T1-weighted images otherwise.

*[FLASH MRI]: Fast low angle shot magnetic resonance imaging

{{ source }}

"""

recreate_bem: bool = False
"""
Whether to re-create the BEM surfaces, even if existing surfaces have been
found. If ``False``, the BEM surfaces are only created if they do not exist
already. ``True`` forces their recreation, overwriting existing BEM surfaces.

{{ source }}

"""

mri_t1_path_generator: Optional[Callable] = None
"""
To perform source-level analyses, the Pipeline needs to generate a
transformation matrix that translates coordinates from MEG and EEG sensor
space to MRI space, and vice versa. This process, called "coregistration",
requires access to both, the electrophyisiological recordings as well as
T1-weighted MRI images of the same participant. If both are stored within
the same session, the Pipeline (or, more specifically, MNE-BIDS) can find the
respective files automatically.

However, in certain situations, this is not possible. Examples include:

- MRI was conducted during a different session than the electrophysiological
  recording.
- MRI was conducted in a single session, while electrophysiological recordings
  spanned across several sessions.
- MRI and electrophysiological data are stored in separate BIDS datasets to
  allow easier storage and distribution in certain situations.

To allow the Pipeline to find the correct MRI images and perform coregistration
automatically, we provide a "hook" that allows you to provide in a custom
function whose output tells the Pipeline where to find the T1-weighted image.

The function is expected to accept a single parameter. The Pipeline will pass
a `BIDSPath` with the following parameters set based on the currently processed
electrophysiological data:

- the subject ID, `BIDSPath.subject`
- the experimental session, `BIDSPath.session`
- the BIDS root, `BIDSPath.root`

This `BIDSPath` can then be modified – or an entirely new `BIDSPath` can be
generated – and returned by the function, pointing to the T1-weighted image.

Note: Note
    The function accepts and returns a single `BIDSPath`.

???+ example "Example"
    The MRI session is different than the electrophysiological session:
    ```python
    def get_t1_from_meeg(bids_path):
        bids_path.session = 'MRI'
        return bids_path


    mri_t1_path_generator = get_t1_from_meeg
    ```

    The MRI recording is stored in a different BIDS dataset than the
    electrophysiological data:
    ```python
    def get_t1_from_meeg(bids_path):
        bids_path.root = '/data/mri'
        return bids_path


    mri_t1_path_generator = get_t1_from_meeg
    ```
{{ source }}

"""

spacing: Union[Literal['oct5', 'oct6', 'ico4', 'ico5', 'all'], int] = 'oct6'
"""
The spacing to use. Can be ``'ico#'`` for a recursively subdivided
icosahedron, ``'oct#'`` for a recursively subdivided octahedron,
``'all'`` for all points, or an integer to use approximate
distance-based spacing (in mm). See (the respective MNE-Python documentation)
[https://mne.tools/dev/overview/cookbook.html#setting-up-the-source-space]
for more info.

{{ source }}

"""

mindist: float = 5
"""
Exclude points closer than this distance (mm) to the bounding surface.

{{ source }}

"""

# loose: Union[float, Literal['auto']] = 0.2
# # ``loose`` : float in [0, 1] | 'auto'
# """
# Value that weights the source variances of the dipole components
# that are parallel (tangential) to the cortical surface. If ``0``, then the
# inverse solution is computed with **fixed orientation.**
# If ``1``, it corresponds to **free orientation.**
# The default value, ``'auto'``, is set to ``0.2`` for surface-oriented source
# spaces, and to ``1.0`` for volumetric, discrete, or mixed source spaces,
# unless ``fixed is True`` in which case the value 0. is used.
# """

# depth: Optional[Union[float, dict]] = 0.8
# """
# If float (default 0.8), it acts as the depth weighting exponent (``exp``)
# to use (must be between 0 and 1). None is equivalent to 0, meaning no
# depth weighting is performed. Can also be a `dict` containing additional
# keyword arguments to pass to :func:`mne.forward.compute_depth_prior`
# (see docstring for details and defaults).
# """

inverse_method: Literal['MNE', 'dSPM', 'sLORETA', 'eLORETA'] = 'dSPM'
"""
Use minimum norm, dSPM (default), sLORETA, or eLORETA to calculate the inverse
solution.

{{ source, report }}

"""

noise_cov: Union[Tuple[Optional[float], Optional[float]],
                 Literal['emptyroom']] = (None, 0)
"""
Specify how to estimate the noise covariance matrix, which is used in
inverse modeling.

If a tuple, it takes the form ``(tmin, tmax)`` with the time specified in
seconds. If the first value of the tuple is ``None``, the considered
period starts at the beginning of the epoch. If the second value of the
tuple is ``None``, the considered period ends at the end of the epoch.
The default, ``(None, 0)``, includes the entire period before the event,
which is typically the pre-stimulus period.

If ``emptyroom``, the noise covariance matrix will be estimated from an
empty-room MEG recording. The empty-room recording will be automatically
selected based on recording date and time.

Please note that when processing data that contains EEG channels, the noise
covariance can ONLY be estimated from the pre-stimulus period.

???+ example "Example"
    Use the period from start of the epoch until 100 ms before the experimental
    event:
    ```python
    noise_cov = (None, -0.1)
    ```

    Use the time period from the experimental event until the end of the epoch:
    ```python
    noise_cov = (0, None)
    ```

    Use an empty-room recording:
    ```python
    noise_cov = 'emptyroom'
    ```
{{ source }}

"""

###############################################################################
# ADVANCED
# --------

l_trans_bandwidth: Union[float, Literal['auto']] = 'auto'
"""
Specifies the transition bandwidth of the
highpass filter. By default it's `'auto'` and uses default MNE
parameters.

{{ preprocessing }}
"""

h_trans_bandwidth: Union[float, Literal['auto']] = 'auto'
"""
Specifies the transition bandwidth of the
lowpass filter. By default it's `'auto'` and uses default MNE
parameters.

{{ preprocessing }}
"""

N_JOBS: int = 1
"""
Specifies how many subjects you want to process in parallel.

{{ init, preprocessing, sensor, source, report }}
"""

random_state: Optional[int] = 42
"""
You can specify the seed of the random number generator (RNG).
This setting is passed to the ICA algorithm and to the decoding function,
ensuring reproducible results. Set to ``None`` to avoid setting the RNG
to a defined state.

{{ preprocessing, sensor }}
"""

shortest_event: int = 1
"""
Minimum number of samples an event must last. If the
duration is less than this, an exception will be raised.

{{ no }}
"""

log_level: Literal['info', 'error'] = 'info'
"""
Set the pipeline logging verbosity.

{{ no }}

"""

mne_log_level: Literal['info', 'error'] = 'error'
"""
Set the MNE-Python logging verbosity.

{{ no }}

"""

on_error: Literal['continue', 'abort'] = 'abort'
"""
Whether to abort processing as soon as an error occurs, or whether to
continue with all other processing steps for as long as possible.

{{ no }}

"""

###############################################################################
#                                                                             #
#                      CUSTOM CONFIGURATION ENDS HERE                         #
#                                                                             #
###############################################################################

###############################################################################
# Name, version, and hosting location of the pipeline
# ---------------------------------------------------

PIPELINE_NAME = 'mne-bids-pipeline'
VERSION = '0.1.dev0'
CODE_URL = 'https://github.com/mne-tools/mne-bids-pipeline'

###############################################################################
# Logger
# ------

logger = logging.getLogger('mne-bids-pipeline')

log_fmt = '%(asctime)s %(message)s'
log_date_fmt = coloredlogs.DEFAULT_DATE_FORMAT = '%H:%M:%S'
coloredlogs.install(level=log_level, logger=logger, fmt=log_fmt,
                    date_fmt=log_date_fmt)

mne.set_log_level(verbose=mne_log_level.upper())

###############################################################################
# Retrieve custom configuration options
# -------------------------------------
#
# For testing a specific dataset, create a Python file with a name of your
# liking (e.g., ``mydataset-template-config.py``), and set an environment
# variable ``MNE_BIDS_STUDY_CONFIG`` to that file.
#
# Example
# ~~~~~~~
# ``export MNE_BIDS_STUDY_CONFIG=/data/mystudy/mydataset-template-config.py``

if "MNE_BIDS_STUDY_CONFIG" in os.environ:
    cfg_path = pathlib.Path(os.environ['MNE_BIDS_STUDY_CONFIG'])

    if cfg_path.exists():
        msg = f'Using custom configuration: {cfg_path}'
        logger.info(msg)
    else:
        msg = ('The custom configuration file specified in the '
               'MNE_BIDS_STUDY_CONFIG environment variable could not be '
               'found: {cfg_path}'.format(cfg_path=cfg_path))
        raise ValueError(msg)

    # Import configuration from an arbitrary path without having to fiddle
    # with `sys.path`.
    spec = importlib.util.spec_from_file_location(name='custom_config',
                                                  location=cfg_path)
    custom_cfg = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(custom_cfg)
    del spec, cfg_path

    new = None
    for val in dir(custom_cfg):
        if not val.startswith('__'):
            exec("new = custom_cfg.%s" % val)
            logger.debug('Overwriting: %s -> %s' % (val, new))
            exec("%s = custom_cfg.%s" % (val, val))


###############################################################################
# CHECKS
# ------

if (use_maxwell_filter and
        len(set(ch_types).intersection(('meg', 'grad', 'mag'))) == 0):
    raise ValueError('Cannot use maxwell filter without MEG channels.')

if (spatial_filter == 'ica' and
        ica_algorithm not in ('picard', 'fastica', 'extended_infomax')):
    msg = (f"Invalid ICA algorithm requested. Valid values for ica_algorithm "
           f"are: 'picard', 'fastica', and 'extended_infomax', but received "
           f"{ica_algorithm}.")
    raise ValueError(msg)

if (spatial_filter == 'ica' and
        ica_l_freq is None and
        l_freq is not None and l_freq < 1):
    msg = (f'You requested to high-pass filter your data with l_freq={l_freq} '
           f'Hz and to perform ICA without performing any additional '
           f'filtering first by setting ica_l_freq=None. However, ICA will '
           f'not work reliably unless slow drifts have been removed from the '
           f'data. Please either increase l_freq to 1 Hz or above, or enable '
           f'additional filtering for ICA by setting ica_l_freq to 1 Hz or '
           f'higher.')
    raise ValueError(msg)

if spatial_filter == 'ica' and ica_l_freq < 1:
    msg = (f'You requested to high-pass filter the data before ICA with '
           f'ica_l_freq={ica_l_freq} Hz. Please increase this setting to '
           f'1 Hz or above to ensure reliable ICA function.')
    raise ValueError(msg)

if (spatial_filter == 'ica' and
        ica_l_freq is not None and
        l_freq is not None and
        ica_l_freq < l_freq):
    msg = (f'You requested a lower high-pass filter cutoff frequency for ICA '
           f'than for your raw data: ica_l_freq = {ica_l_freq} < '
           f'l_freq = {l_freq}. Adjust the cutoffs such that ica_l_freq >= '
           f'l_freq, or set ica_l_freq to None if you do not wish to apply '
           f'an additional high-pass filter before running ICA.')
    raise ValueError(msg)

if not ch_types:
    msg = 'Please specify ch_types in your configuration.'
    raise ValueError(msg)

if ch_types == ['eeg']:
    pass
elif 'eeg' in ch_types and len(ch_types) > 1:  # EEG + some other channel types
    msg = ('EEG data can only be analyzed separately from other channel '
           'types. Please adjust `ch_types` in your configuration.')
    raise ValueError(msg)
elif any([ch_type not in ('meg', 'mag', 'grad') for ch_type in ch_types]):
    msg = ('Invalid channel type passed. Please adjust `ch_types` in your '
           'configuration.')
    raise ValueError(msg)

if 'eeg' in ch_types:
    if spatial_filter == 'ssp':
        msg = ("You requested SSP for EEG data via spatial_filter='ssp'. "
               "However, this is not presently supported. Please use ICA "
               "instead by setting spatial_filter='ica'.")
        raise ValueError(msg)

if conditions is None:
    msg = ('Please indicate the name of your conditions in your configuration. '
           'Currently the `conditions` parameter is empty.')
    raise ValueError(msg)

if on_error not in ('continue', 'abort', 'debug'):
    msg = (f"on_error must be one of 'continue', 'debug' or 'abort', "
           f"but received: {on_error}.")
    logger.info(msg)

if isinstance(noise_cov, str) and noise_cov != 'emptyroom':
    msg = (f"noise_cov must be a tuple or 'emptyroom', but received "
           f"{noise_cov}")
    raise ValueError(msg)

if noise_cov == 'emptyroom' and 'eeg' in ch_types:
    msg = ('You requested to process data that contains EEG channels. In this '
           'case, noise covariance can only be estimated from the '
           'experimental data, e.g., the pre-stimulus period. Please set '
           'noise_cov to (tmin, tmax)')
    raise ValueError(msg)

if noise_cov == 'emptyroom' and not process_er:
    msg = ('You requested noise covariance estimation from empty-room '
           'recordings by setting noise_cov = "emptyroom", but you did not '
           'enable empty-room data processing. Please set process_er = True')
    raise ValueError(msg)

if bem_mri_images not in ('FLASH', 'T1', 'auto'):
    msg = (f'Unknown bem_mri_images: {bem_mri_images}. Valid values '
           f'are: "FLASH", "T1", and "auto".')
    raise ValueError(msg)


###############################################################################
# Helper functions
# ----------------

def get_bids_root() -> pathlib.Path:
    # BIDS_ROOT environment variable takes precedence over any configuration file
    # values.
    root = os.getenv('BIDS_ROOT')
    if root is not None:
        return (pathlib.Path(root)
                .expanduser()
                .resolve(strict=True))

    # If we don't have a bids_root until now, raise an exception as we cannot
    # proceed.
    if not bids_root:
        msg = ('You need to specify `bids_root` in your configuration, or '
               'define an environment variable `BIDS_ROOT` pointing to the '
               'root folder of your BIDS dataset')
        raise ValueError(msg)

    return (pathlib.Path(bids_root)
            .expanduser()
            .resolve(strict=True))


def get_deriv_root() -> pathlib.Path:
    if deriv_root is None:
        return get_bids_root() / 'derivatives' / PIPELINE_NAME
    else:
        return (pathlib.Path(deriv_root)
                .expanduser()
                .resolve())


def get_sessions():
    sessions_ = copy.deepcopy(sessions)  # Avoid clash with global variable.

    env = os.environ
    if env.get('MNE_BIDS_STUDY_SESSION'):
        sessions_ = env['MNE_BIDS_STUDY_SESSION']
    elif sessions_ == 'all':
        sessions_ = get_entity_vals(bids_root, entity_key='session')

    if not sessions_:
        return [None]
    else:
        return sessions_


def get_runs() -> list:
    runs_ = copy.deepcopy(runs)  # Avoid clash with global variable.
    valid_runs = get_entity_vals(get_bids_root(), entity_key='run')

    env_run = os.environ.get('MNE_BIDS_STUDY_RUN')
    if env_run and env_run not in valid_runs:
        raise ValueError(
            f'Invalid run. It can be {valid_runs} but '
            f'got {env_run}')
    elif env_run:
        runs_ = [env_run]
    elif runs_ == 'all':
        runs_ = valid_runs

    if not runs_:
        return [None]
    else:
        return runs_


# XXX This check should actually go into the CHECKS section, but it depends
# XXX on get_runs(), which is defined after that section.
if mf_reference_run is not None and mf_reference_run not in get_runs():
    msg = (f'You set mf_reference_run={mf_reference_run}, but your dataset '
           f'only contains the following runs: {get_runs()}')
    raise ValueError(msg)


def get_mf_reference_run() -> str:
    # Retrieve to run identifier (number, name) of the reference run
    if mf_reference_run is None:
        # Use the first run
        return get_runs()[0]
    else:
        return mf_reference_run


def get_subjects() -> List[str]:
    global subjects

    env = os.environ

    valid_subjects = get_entity_vals(get_bids_root(), entity_key='subject')

    if env.get('MNE_BIDS_STUDY_SUBJECT'):
        env_subject = env['MNE_BIDS_STUDY_SUBJECT']
        if env_subject not in valid_subjects:
            raise ValueError(
                f'Invalid subject. It can be {valid_subjects} but '
                f'got {env_subject}')
        s = [env_subject]
    elif subjects == 'all':
        s = valid_subjects
    else:
        s = subjects

    subjects = set(s) - set(exclude_subjects)
    # Drop empty-room subject.
    subjects = subjects - set(['emptyroom'])

    return sorted(subjects)


def get_task() -> Optional[str]:
    global task

    env = os.environ
    valid_tasks = get_entity_vals(get_bids_root(), entity_key='task')

    if env.get('MNE_BIDS_STUDY_TASK'):
        task = env['MNE_BIDS_STUDY_TASK']
        if task not in valid_tasks:
            raise ValueError(f'Invalid task. It can be: '
                             f'{", ".join(valid_tasks)} but got: {task}')

    if not task:
        if not valid_tasks:
            return None
        else:
            return valid_tasks[0]
    else:
        return task


def get_datatype() -> Literal['meg', 'eeg']:
    # Content of ch_types should be sanitized already, so we don't need any
    # extra sanity checks here.
    if data_type is not None:
        return data_type
    elif data_type is None and ch_types == ['eeg']:
        return 'eeg'
    elif data_type is None and any([t in ['meg', 'mag', 'grad']
                                    for t in ch_types]):
        return 'meg'
    else:
        raise RuntimeError("This probably shouldn't happen. Please contact "
                           "the MNE-BIDS-pipeline developers. Thank you.")


def _get_reject(
    reject: Optional[Dict[str, float]],
    ch_types: Iterable[Literal['meg', 'mag', 'grad', 'eeg']]
) -> Dict[str, float]:
    if reject is None:
        return dict()

    reject = reject.copy()

    if ch_types == ['eeg']:
        ch_types_to_remove = ('mag', 'grad')
    else:
        ch_types_to_remove = ('eeg',)

    for ch_type in ch_types_to_remove:
        try:
            del reject[ch_type]
        except KeyError:
            pass

    return reject


def get_reject() -> Dict[str, float]:
    return _get_reject(reject=reject, ch_types=ch_types)


def get_ica_reject() -> Dict[str, float]:
    return _get_reject(reject=ica_reject, ch_types=ch_types)


def get_fs_subjects_dir():
    if not subjects_dir:
        return get_bids_root() / 'derivatives' / 'freesurfer' / 'subjects'
    else:
        return subjects_dir


def gen_log_message(message, step=None, subject=None, session=None,
                    run=None) -> str:
    if subject is not None:
        subject = f'sub-{subject}'
    if session is not None:
        session = f'ses-{session}'
    if run is not None:
        run = f'run-{run}'

    prefix = ', '.join([item for item in [subject, session, run]
                        if item is not None])
    if prefix:
        prefix = f'[{prefix}]'

    if step is not None:
        prefix = f'[Step-{step:02}]{prefix}'

    return prefix + ' ' + message


def failsafe_run(on_error):
    def failsafe_run_decorator(func):
        @functools.wraps(func)  # Preserve "identity" of original function
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                message = 'A critical error occurred.'
                message = gen_log_message(message=message)

                if on_error == 'abort':
                    logger.critical(message)
                    raise(e)
                elif on_error == 'debug':
                    logger.critical(message)
                    extype, value, tb = sys.exc_info()
                    traceback.print_exc()
                    pdb.post_mortem(tb)
                else:
                    message = f'{message} The error message was:\n{str(e)}'
                    logger.critical(message)
        return wrapper
    return failsafe_run_decorator


def plot_auto_scores(auto_scores):
    # Plot scores of automated bad channel detection.
    import matplotlib.pyplot as plt
    import seaborn as sns
    import pandas as pd

    if ch_types == ['meg']:
        ch_types_ = ['grad', 'mag']
    else:
        ch_types_ = ch_types

    figs = []
    for ch_type in ch_types_:
        # Only select the data for mag or grad channels.
        ch_subset = auto_scores['ch_types'] == ch_type
        ch_names = auto_scores['ch_names'][ch_subset]
        scores = auto_scores['scores_noisy'][ch_subset]
        limits = auto_scores['limits_noisy'][ch_subset]
        bins = auto_scores['bins']  # The the windows that were evaluated.

        # We will label each segment by its start and stop time, with up to 3
        # digits before and 3 digits after the decimal place (1 ms precision).
        bin_labels = [f'{start:3.3f} – {stop:3.3f}'
                      for start, stop in bins]

        # We store the data in a Pandas DataFrame. The seaborn heatmap function
        # we will call below will then be able to automatically assign the
        # correct labels to all axes.
        data_to_plot = pd.DataFrame(data=scores,
                                    columns=pd.Index(bin_labels,
                                                     name='Time (s)'),
                                    index=pd.Index(ch_names, name='Channel'))

        # First, plot the "raw" scores.
        fig, ax = plt.subplots(1, 2, figsize=(12, 8))
        fig.suptitle(f'Automated noisy channel detection: {ch_type}',
                     fontsize=16, fontweight='bold')
        sns.heatmap(data=data_to_plot, cmap='Reds',
                    cbar_kws=dict(label='Score'), ax=ax[0])
        [ax[0].axvline(x, ls='dashed', lw=0.25, dashes=(25, 15), color='gray')
            for x in range(1, len(bins))]
        ax[0].set_title('All Scores', fontweight='bold')

        # Now, adjust the color range to highlight segments that exceeded the
        # limit.
        sns.heatmap(data=data_to_plot,
                    vmin=np.nanmin(limits),  # input data may contain NaNs
                    cmap='Reds', cbar_kws=dict(label='Score'), ax=ax[1])
        [ax[1].axvline(x, ls='dashed', lw=0.25, dashes=(25, 15), color='gray')
            for x in range(1, len(bins))]
        ax[1].set_title('Scores > Limit', fontweight='bold')

        # The figure title should not overlap with the subplots.
        fig.tight_layout(rect=[0, 0.03, 1, 0.95])
        figs.append(fig)

    return figs


def get_channels_to_analyze(info) -> List[str]:
    # Return names of the channels of the channel types we wish to analyze.
    # We also include channels marked as "bad" here.
    # `exclude=[]`: keep "bad" channels, too.
    if get_datatype() == 'meg' and ('mag' in ch_types or 'grad' in ch_types
                                    or 'meg' in ch_types):
        pick_idx = mne.pick_types(info, eog=True, ecg=True, exclude=[])

        if 'mag' in ch_types:
            pick_idx += mne.pick_types(info, meg='mag', exclude=[])
        if 'grad' in ch_types:
            pick_idx += mne.pick_types(info, meg='grad', exclude=[])
        if 'meg' in ch_types:
            pick_idx = mne.pick_types(info, meg=True, eog=True, ecg=True,
                                      exclude=[])
    elif ch_types == ['eeg']:
        pick_idx = mne.pick_types(info, meg=False, eeg=True, eog=True,
                                  ecg=True, exclude=[])
    else:
        raise RuntimeError('Something unexpected happened. Please contact '
                           'the mne-bids-pipeline developers. Thank you.')

    ch_names = [info['ch_names'][i] for i in pick_idx]
    return ch_names


def get_fs_subject(subject) -> str:
    subjects_dir = get_fs_subjects_dir()

    if (pathlib.Path(subjects_dir) / subject).exists():
        return subject
    else:
        return f'sub-{subject}'


def sanitize_cond_name(cond: str) -> str:
    cond = (cond
            .replace(os.path.sep, '')
            .replace('_', '')
            .replace('-', ''))
    return cond
